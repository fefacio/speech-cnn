# -*- coding: utf-8 -*-
"""speech accent.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1d-NYthYBFtqDxkiQw6IR-QYQVVS4PaUS
"""

# Libraries
import time
import os

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split

import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import transforms, models
from torch.utils.data import DataLoader


import librosa

# File paths
RECORDINGS_PATH = './recordings/recordings/'
RECORDINGS_COMPRESSED_PATH = './recordings_compressed/'
DATASET_PATH = './speakers_all.csv'
SPEC_PATH = './spectrogramas/'
RESULTS_PATH = './results/'


"""## CNN"""

def save_spectrogram(audio_file, save_path,  duration_sec, sr=16000, hop_length=160, n_mels=128):
    try:
        y, _ = librosa.load(audio_file, sr=sr)
        y = y[:sr * duration_sec]
        mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, hop_length=hop_length, n_mels=n_mels)
        log_mel_spec = librosa.power_to_db(mel_spec, ref=np.max)

        # Size adjusted for CNN
        plt.figure(figsize=(2.24, 2.24))
        librosa.display.specshow(log_mel_spec, sr=sr, hop_length=hop_length, x_axis=None, y_axis=None)
        plt.axis('off')
        plt.tight_layout(pad=0)

        # Gera nome de arquivo baseado no nome do áudio
        base_name = os.path.splitext(os.path.basename(audio_file))[0]
        plt.savefig(os.path.join(save_path, base_name + '.png'), bbox_inches='tight', pad_inches=0)
        plt.close()
    except Exception as e:
        print(f"Erro no arquivo {audio_file}: {e}")


def save_all_spectrogram_dir(recordings_dir, save_dir):
    print(f'Starting saving audio spectrograms from {recordings_dir}')
    os.makedirs(save_dir, exist_ok=True)

    # Itera por todos os .mp3 na pasta
    for filename in os.listdir(recordings_dir):
        if filename.endswith('.mp3'):
            file_path = os.path.join(recordings_dir, filename)
            save_spectrogram(file_path, save_dir, duration_sec=5)

    print("Espectrogramas salvos em:", save_dir)

def save_spectrogram_df(df, recordings_dir, save_dir):
    print(f'Starting saving audio spectrograms from {recordings_dir}')
    os.makedirs(save_dir, exist_ok=True)
    file_names = list(df['filename'].values)
    
    # Itera por todos os .mp3 na pasta
    for filename in os.listdir(recordings_dir):
        if filename.endswith('.mp3') and (filename in file_names):
            file_path = os.path.join(recordings_dir, filename)
            save_spectrogram(file_path, save_dir, duration_sec=20)

    print("Espectrogramas salvos em:", save_dir)

from PIL import Image


################################
#         Speech Dataset       #
################################

class SpeechDataset(torch.utils.data.Dataset):
    def __init__(self, df: pd.DataFrame, images_path: str, target: str,
                  transform = None, file_extension = '.png'):
        self.df = df
        self.images_path = images_path
        self.target = target
        self.transform = transform
        self.file_extension = file_extension

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        
        image_path = os.path.join(self.images_path, 
                                  row['filename'] + self.file_extension)

        image = Image.open(image_path).convert('RGB')
        label = torch.tensor([row[self.target]], dtype=torch.float)

        if self.transform:
            image = self.transform(image)

        return image, label
    
    def dataset_clean(self):
        print(f'Size before clean {len(self.df)}')
        # Remove columns that have 80% of missing (NaN) values
        self.df = self.df.dropna(axis=1, thresh=0.8*len(self.df))
        print(f'Size after clean {len(self.df)}')
        # Remove data entries that have no associated recording
        self.df = self.df[self.df['file_missing?'] != True]
        print(f'Size after clean {len(self.df)}')
        # Fix typo in 'sex' column
        self.df['sex'] = self.df['sex'].replace('famale', 'female')
        print(f'Size after clean {len(self.df)}')
        # Dataset has 2140 entries but we have only 2128 mp3 files
        existing_files = [file.split('.')[0] for file in os.listdir(RECORDINGS_PATH)]  # lista de arquivos existentes na pasta
        existing_files
        # Remove rows that have missing recordings files
        self.df = self.df[self.df['filename'].isin(existing_files)]
        

        print(f'Size after clean {len(self.df)}')

    def map_target_to_binary(self, target: str):
        self.df = self.df.dropna(subset=[target])
        target_names: list[str] = self.df[target].unique()
        binary_mapping = {
            target_names[0]: 1,
            target_names[1]: 0,
        }
        new_target_name = 'is'+target_names[0].capitalize()
        self.df[new_target_name] = self.df[target].map(binary_mapping)
        self.df = self.df.drop(columns=[target])
        return new_target_name
        

#####################################################################
#####################################################################
#####################################################################
#####################################################################

#####################################
# Training and validation functions #
#####################################

def get_vgg16():
    model = models.vgg16(weights=models.VGG16_Weights.DEFAULT)

    # Primeiro, congelamos tudo
    for param in model.parameters():
        param.requires_grad = False

    # Descongelar as últimas 8 camadas (contando convolucionais e lineares)
    # Vamos contar a partir do fim da lista de parâmetros treináveis
    trainable_params = list(model.features.parameters()) + list(model.classifier.parameters())
    for param in trainable_params[-6:]:
        param.requires_grad = True

    # Substituir o avgpool por pooling adaptativo 1x1
    model.avgpool = nn.AdaptiveAvgPool2d(output_size=(1,1))

    # Substituir o classificador por um customizado para classificação binária
    model.classifier = nn.Sequential(
        nn.Flatten(),
        nn.Linear(512, 128),
        nn.ReLU(),
        nn.Dropout(0.2),
        nn.Linear(128, 1)  # sem sigmoid pois vamos usar BCEWithLogitsLoss
    )

    # Como modificamos o classificador, garantimos que seus parâmetros sejam treináveis
    for param in model.classifier.parameters():
        param.requires_grad = True

    loss_fn = nn.BCEWithLogitsLoss()
    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-3)

    return model, loss_fn, optimizer


def get_resnet50():
    model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)

    # Congelar todos os parâmetros inicialmente
    for param in model.parameters():
        param.requires_grad = False

    # Descongelar as camadas da layer4 (último bloco residual)
    for param in model.layer4.parameters():
        param.requires_grad = True

    # Substituir o classificador (fc) por um para saída binária
    model.fc = nn.Sequential(
        nn.Linear(model.fc.in_features, 128),
        nn.ReLU(),
        nn.Dropout(0.3),
        nn.Linear(128, 1)  # sem sigmoid, usa BCEWithLogitsLoss
    )

    # Garantir que o novo classificador seja treinável
    for param in model.fc.parameters():
        param.requires_grad = True

    # Loss e otimizador
    loss_fn = nn.BCEWithLogitsLoss()
    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)

    return model, loss_fn, optimizer


def train(model, train_loader, loss_fn, optimizer):
  model.train()
  epoch_losses = []
  epoch_accuracies = []

  for x, y in train_loader:
        # Forward pass
        prediction = model(x)

        # Calculate loss
        #y = y.squeeze().long()
        # print(f'pred: {prediction}')
        # print(f'y: {y}')
        batch_loss = loss_fn(prediction, y)
        epoch_losses.append(batch_loss.item())

        # Calculate accuracy
        is_correct = (prediction > 0.5).int() == y.int()
        #preds = prediction.argmax(dim=1)     # predicted class
        #is_correct = (preds == y)
        epoch_accuracies.extend(is_correct.cpu().numpy())

        # Backward pass and optimization
        optimizer.zero_grad()
        batch_loss.backward()
        optimizer.step()

  # Calculate mean loss and accuracy for this epoch
  epoch_loss = np.mean(epoch_losses)
  epoch_accuracy = np.mean(epoch_accuracies)

  return epoch_loss, epoch_accuracy


def evaluate(model, validation_loader, loss_fn):
    model.eval()
    epoch_losses = []
    epoch_accuracies = []

    with torch.no_grad():
        for x, y in validation_loader:
            # Forward pass
            prediction = model(x)

            # y = y.squeeze().long()
            # Calculate loss
            val_loss = loss_fn(prediction, y)
            epoch_losses.append(val_loss.item())

            # Calculate accuracy
            is_correct = (prediction > 0.5).int() == y.int()
            #preds = prediction.argmax(dim=1)     # predicted class
            #is_correct = (preds == y)
            epoch_accuracies.extend(is_correct.cpu().numpy())

    # Calculate mean loss and accuracy for validation
    epoch_loss = np.mean(epoch_losses)
    epoch_accuracy = np.mean(epoch_accuracies)

    return epoch_loss, epoch_accuracy




def main():
    # save_all_spectrogram_dir(RECORDINGS_PATH, SPEC_PATH)

    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),  # converte de 0-255 para 0-1 e rearranja para (C, H, W)
        transforms.Normalize([0.485, 0.456, 0.406],
                            [0.229, 0.224, 0.225])
    ])

    df = pd.read_csv(DATASET_PATH)

    # usa_df = df[df['country'] == 'usa']
    # non_usa_df = df[df['country'] != 'usa']
    # non_usa_df = non_usa_df.sample(len(usa_df), random_state=42)
    # df = pd.concat([non_usa_df, usa_df], ignore_index=True)

    # df['isUsa'] = (df['country'] == 'usa').astype(int)
    # allowed_countries = {'usa', 'canada', 'uk', 'australia'}
    # df = df[
    #     (df['country'].str.lower().isin(allowed_countries)) &
    #     (df['native_language'].str.lower() == 'english')
    # ]
    # # Apeend file extension to filename
    # df['filename'] = df['filename'] + '.mp3'
    # classes = {
    #     'usa': 0,
    #     'canada': 1,
    #     'uk': 2,
    #     'australia': 3
    # }
    target = 'sex'
    # speech. df[target] = df[target].map(classes)
    # save_spectrogram_df(df, RECORDINGS_PATH, SPEC_PATH+'usa20/')
    speech = SpeechDataset(df,
                        SPEC_PATH,
                        target,
                        transform)
    speech.dataset_clean()
    
    new_target = speech.map_target_to_binary(target)
    # new_target = 'isUsa'
    # model, loss_fn, optimizer = get_vgg16()

    # Perform train-test split
    train_df, test_df = train_test_split(speech.df, 
                                         test_size=0.2, 
                                         stratify=speech.df[new_target], 
                                         random_state=42)

    # Train and test datasets
    train_df = SpeechDataset(train_df, images_path=SPEC_PATH,
                              target=new_target, transform=transform)
    test_df = SpeechDataset(test_df, images_path=SPEC_PATH, 
                            target=new_target, transform=transform)
    
    # DataLoaders
    train_loader = DataLoader(train_df, batch_size=32, shuffle=True)
    test_loader = DataLoader(test_df, batch_size=32, shuffle=False)


    
    ################################
    # Training and validation loop #
    ################################
    print(len(speech.df[speech.df[new_target] == 0]))
    print(len(speech.df[speech.df[new_target] == 1]))

    model, loss_fn, optimizer = get_resnet50()
    print("Starting train-validation loop...")
    result_csv_path = os.path.join(RESULTS_PATH, 'resnet50_sex.csv')
    columns = [
        'epoch', 'time', 'train_loss', 'train_acc', 'val_loss', 'val_acc'
    ]
    pd.DataFrame(columns=columns).to_csv(result_csv_path, index=False)

    epochs = 10
    for epoch in range(epochs):
        print(f"Epoch {epoch + 1}/{epochs}")

        start = time.time()
        # Train for one epoch
        train_loss, train_acc = train(model, train_loader, loss_fn, optimizer)

        # Evaluate on the validation set
        val_loss, val_acc = evaluate(model, test_loader, loss_fn)
        end = time.time()
        elapsed_time = end- start

        # Store metrics for plotting or logging
        result = [epoch + 1, 
                  elapsed_time, 
                  train_loss, 
                  train_acc, 
                  val_loss, 
                  val_acc]
        pd.DataFrame([result], columns=columns).to_csv(
            result_csv_path, mode='a', index=False, header=False
        )

    # model, loss_fn, optimizer = get_vgg16()
    # print("Starting train-validation loop...")
    # result_csv_path = os.path.join(RESULTS_PATH, 'vgg16_usa_ft6-inf.csv')
    # columns = [
    #     'epoch', 'time', 'train_loss', 'train_acc', 'val_loss', 'val_acc'
    # ]
    # pd.DataFrame(columns=columns).to_csv(result_csv_path, index=False)

    # epochs = 100
    # for epoch in range(epochs):
    #     print(f"Epoch {epoch + 1}/{epochs}")

    #     start = time.time()
    #     # Train for one epoch
    #     train_loss, train_acc = train(model, train_loader, loss_fn, optimizer)

    #     # Evaluate on the validation set
    #     val_loss, val_acc = evaluate(model, test_loader, loss_fn)
    #     end = time.time()
    #     elapsed_time = end- start

    #     # Store metrics for plotting or logging
    #     result = [epoch + 1, 
    #               elapsed_time, 
    #               train_loss, 
    #               train_acc, 
    #               val_loss, 
    #               val_acc]
    #     pd.DataFrame([result], columns=columns).to_csv(
    #         result_csv_path, mode='a', index=False, header=False
    #     )


if __name__ == "__main__":
    main()


